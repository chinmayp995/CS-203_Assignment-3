{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Dataset\n",
    "\n",
    "For the POS part, both the members of our team separate annotations for the provided CSV sentences between 381 to 400 as per our team number. Thereafter; we used the LabelStudio to annotate properly and then we exported the files. Thereafter; we just cleaned the values using clean_data.py code such that only the required data is kept then we started our work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.8985789913624965\n"
     ]
    }
   ],
   "source": [
    "# importing the required libraries\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "\n",
    "# Reading the POS tags from both annotators\n",
    "annotator1 = pd.read_csv(r'C:\\Users\\Admin\\Desktop\\iitgn\\CS-203_Assignment-3\\Task 2\\cleaned_data\\NLP_Anntn1.csv')\n",
    "annotator2 = pd.read_csv(r'C:\\Users\\Admin\\Desktop\\iitgn\\CS-203_Assignment-3\\Task 2\\cleaned_data\\NLP_Anntn2.csv')\n",
    "\n",
    "# Extracting and processing the POS tags\n",
    "def extract_labels(label):\n",
    "    return [item['label'] for item in ast.literal_eval(label)]\n",
    "\n",
    "annotator1['labels'] = annotator1['POS tags'].apply(extract_labels)\n",
    "annotator2['labels'] = annotator2['POS tags'].apply(extract_labels)\n",
    "\n",
    "# this assert statement Ensures that both annotators are comparing the same words in the same order\n",
    "assert annotator1['Sentences'].equals(annotator2['Sentences'])\n",
    "\n",
    "# Flattening the lists of labels for comparison\n",
    "labels_annotator1 = [label for sublist in annotator1['labels'] for label in sublist]\n",
    "labels_annotator2 = [label for sublist in annotator2['labels'] for label in sublist]\n",
    "\n",
    "# Cohen's Kappa function\n",
    "cohen_kappa = cohen_kappa_score(labels_annotator1, labels_annotator2)\n",
    "print(f\"Cohen's Kappa: {cohen_kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply the Cohen Kappa's code for the same annotator; we get the desired value of 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 1.0\n"
     ]
    }
   ],
   "source": [
    "cohen_kappa = cohen_kappa_score(labels_annotator1, labels_annotator1)\n",
    "print(f\"Cohen's Kappa: {cohen_kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fleiss Kappa\n",
    "For the fleiss Kappaa we made a matrix which combined the data of both the annonators; transposed that accordingly and then counted the required values to generate the frequency matrix which is required for the builtin function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' Kappa: 0.8983861118380877\n"
     ]
    }
   ],
   "source": [
    "# Combining annotations into a matrix for multiple annotators (example assumes two annotators)\n",
    "annotations = [labels_annotator1, labels_annotator2]\n",
    "annotations_matrix = np.array(annotations).T  # Transpose to match Fleiss' format\n",
    "\n",
    "# Converting to a frequency matrix (e.g., how many annotators labeled each word as each category)\n",
    "categories = sorted(set(labels_annotator1 + labels_annotator2))\n",
    "freq_matrix = []\n",
    "for row in annotations_matrix:\n",
    "    freq_matrix.append([list(row).count(category) for category in categories])\n",
    "\n",
    "# Calculate Fleiss' Kappa using the inbuilt fucntion from statsmodels\n",
    "fleiss_kappa_score = fleiss_kappa(freq_matrix)\n",
    "print(f\"Fleiss' Kappa: {fleiss_kappa_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV DataSet\n",
    "For this dataset we didnt required to clean the data as the options were directly present in the column of choice; so we directly converted the csv files into dataframes and then used our code. We thank Paras Shirvale for helping us to annotate our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.5833333333333333\n"
     ]
    }
   ],
   "source": [
    "#Getting our CSV files\n",
    "cv_chinmay=pd.read_csv(r'C:\\Users\\Admin\\Desktop\\iitgn\\CS-203_Assignment-3\\Task 2\\uncleaned_data\\CV_Anntn1.csv')\n",
    "cv_dakshata=pd.read_csv(r'C:\\Users\\Admin\\Desktop\\iitgn\\CS-203_Assignment-3\\Task 2\\uncleaned_data\\CV_Anntn2.csv')\n",
    "cv_paras=pd.read_csv(r'C:\\Users\\Admin\\Desktop\\iitgn\\CS-203_Assignment-3\\Task 2\\uncleaned_data\\CV_Anntn3.csv')\n",
    "#importing the libraries\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "#Calculating the Cohen's Kappa between Chinmay and DakshataTai\n",
    "cohens_kappa = cohen_kappa_score(cv_chinmay['choice'], cv_dakshata['choice'])\n",
    "print(f\"Cohen's Kappa: {cohens_kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.5652173913043478\n"
     ]
    }
   ],
   "source": [
    "#Calculating the Cohen's Kappa between Paras and DakshataTai\n",
    "cohens_kappa = cohen_kappa_score(cv_paras['choice'], cv_dakshata['choice'])\n",
    "print(f\"Cohen's Kappa: {cohens_kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Calculating the Cohen's Kappa between Paras and Paras\n",
    "cohens_kappa = cohen_kappa_score(cv_paras['choice'], cv_paras['choice'])\n",
    "print(f\"Cohen's Kappa: {cohens_kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fliess Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import numpy as np\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "\n",
    "#Preparing the data for Fleiss' Kappa using two annotator's dataframes\n",
    "def prepare_fleiss_data(df1, df2):\n",
    "    unique_categories = sorted(set(df1['choice']).union(set(df2['choice'])))\n",
    "    fleiss_matrix = []\n",
    "    for idx, row in df1.iterrows(): # goes through each row of the dataframe df1\n",
    "        row_counts = [0] * len(unique_categories) #creates a list of 0s of length equal to the number of unique categories\n",
    "        row_counts[unique_categories.index(row['choice'])] += 1  #adds to frequency count of the category\n",
    "        row_counts[unique_categories.index(df2.loc[idx, 'choice'])] += 1 #adds to frequency count of the category\n",
    "        fleiss_matrix.append(row_counts)\n",
    "    return np.array(fleiss_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' Kappa: 0.5698924731182794\n"
     ]
    }
   ],
   "source": [
    "#Getting Freq array for two people\n",
    "fleiss_data = prepare_fleiss_data(cv_chinmay, cv_dakshata)\n",
    "\n",
    "# Calculate Fleiss' Kappa\n",
    "fleiss_kappa_value = fleiss_kappa(fleiss_data)\n",
    "print(f\"Fleiss' Kappa: {fleiss_kappa_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' Kappa: 0.37304075235109707\n"
     ]
    }
   ],
   "source": [
    "# with the third annotator\n",
    "fleiss_data = prepare_fleiss_data(cv_chinmay, cv_paras)\n",
    "\n",
    "# Calculate Fleiss' Kappa\n",
    "fleiss_kappa_value = fleiss_kappa(fleiss_data)\n",
    "print(f\"Fleiss' Kappa: {fleiss_kappa_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' Kappa: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Between Same Person\n",
    "fleiss_data = prepare_fleiss_data(cv_chinmay, cv_chinmay)\n",
    "\n",
    "# Calculate Fleiss' Kappa\n",
    "fleiss_kappa_value = fleiss_kappa(fleiss_data)\n",
    "print(f\"Fleiss' Kappa: {fleiss_kappa_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
